#!/usr/bin/env python
#
# The MIT License (MIT)
#
# Copyright (c) 2015 Christofer Hedbrandh
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import argparse
import itertools

from glabra import text
from glabra import seq_analyze

WORD_DELIM_PATTERN = r"\s|\,|:|;|\""
SENTENCE_DELIM_PATTERN = r"\."

BOUNDS_DEFAULT = ["6:0,100"]
NUMBER_RANDOM_DEFAULT = 10
POST_PROCESSING_FUN = lambda words: " ".join(words) + "."

DESCRIPTION = \
r"""Generate new sentences from one or more files of training data.

A delimiter pattern determines where a sentence starts and where it ends, and
a different delimiter pattern determines where a word starts and ends.

For example, this text may be parsed with the following.

--sentence-delim "\." --word-delim "\s|\,"

"First shalt thou take out the Holy Pin. Then shalt thou count
to three, no more, no less. Three shall be the number thou
shalt count, and the number of the counting shall be three."

Bounds define which n-grams in the training data that will be used to generate
new sentences. The n-grams are specified by length and frequency percentile.

For example all 5-grams in all sentences generated with --bound 5:50,100 will
belong to the 50% most frequent 5-grams in the training data.

Sentences can be generated by either requesting some number of random
sentences or by requesting all sentences with some limit. The lengths of the
sentences generated are based on the lengths of sentences in the training data.

Occasionally sentences that already exist in the training data may be
generated. If "unique" sentences are requested, the returned sentences are
guaranteed to not appear in the training data, and they are guaranteed to not
be returned twice. This means that fewer than the number of requested random
sentences may be returned."""

def main():

  parser = get_arg_parser()
  args = parser.parse_args()

  bounds = text.parse_bounds(args.bounds or BOUNDS_DEFAULT)

  # print help and exit if no files were specified
  if len(args.filenames or []) == 0:
    print "Must specify at least one filename."
    parser.print_help()
    exit(1)

  # decode delim strings passed in from command line
  args.word_delim = args.word_delim.decode("string-escape")
  args.sentence_delim = args.sentence_delim.decode("string-escape")

  # parse files and analyze contents
  sequence_analyzer = get_sequence_analyzer(
    args.filenames, args.sentence_delim, args.word_delim)

  # create the generator by putting a space between each word and ending with '.'
  text_generator = text.TextGenerator(
    bounds, sequence_analyzer, post_processing_fun=POST_PROCESSING_FUN)

  # exit if no sentences can be generated
  if text_generator.is_empty():
    print "Could not generate any sentences. Try larger files or expanding bounds."
    exit(1)

  # print random sentences if all_limit has not been set, else print all
  if args.all_limit == None:
    for sentence in text_generator.get_random_texts(args.num_random, args.unique):
      print sentence
  else:
    for sentence in itertools.islice(text_generator.get_all_texts(args.unique), args.all_limit):
      print sentence

def get_sequence_analyzer(filenames, sentence_delim, word_delim):
  """Parse and analyzer sentence files.

  Files are parsed using the provided patterns. One SequenceAnalyzer is
  created for each file, they are weighted equally and merged.

  Args:
    filenames: Files with sentences separated by sentence_delim.
    sentence_delim: Pattern for sentence separation.
    word_delim: Pattern for word separation.

  Returns:
    A SequenceAnalyzer with all files weighted equally.
  """
  # analyze data files
  sequence_analyzers = []
  for filename in filenames:
    sequence_analyzers.append(text.get_sequence_analyzer(
      filename, sentence_delim, element_delimiter_pattern=word_delim))

  # merge all analyzers into one
  return seq_analyze.merge_normalized_sequence_analyzers(sequence_analyzers)

def get_arg_parser():
  parser = argparse.ArgumentParser(
    formatter_class=argparse.RawDescriptionHelpFormatter,
    description=DESCRIPTION)
  parser.add_argument("--filename",
    dest="filenames",
    metavar="FILENAME",
    action="append",
    help="File with words to parse.")
  parser.add_argument("--word-delim",
    dest="word_delim",
    metavar="WORD_DELIMITER_PATTERN",
    default=WORD_DELIM_PATTERN,
    help="Delimiter pattern for words. (default: %(default)s)")
  parser.add_argument("--sentence-delim",
    dest="sentence_delim",
    metavar="SENTENCE_DELIMITER_PATTERN",
    default=SENTENCE_DELIM_PATTERN,
    help="Delimiter pattern for sentences. (default: %(default)s)")
  parser.add_argument("--bound",
    dest="bounds",
    metavar="BOUND",
    action="append",
    help="One or more bounds for inclusion of n-grams. (default: %s)" % BOUNDS_DEFAULT)
  parser.add_argument("--get-random-sentences",
    dest="num_random",
    metavar="NUMBER_OF_RANDOM_SENTENCES",
    default=NUMBER_RANDOM_DEFAULT,
    type=int,
    help="Number of random sentences to generate. (default: %(default)s)")
  parser.add_argument("--get-all-sentences",
    dest="all_limit",
    metavar="ALL_SENTENCES_LIMIT",
    default=None,
    type=int,
    help="Generate all possible sentences with some limit. (default: %(default)s)")
  parser.add_argument("--unique",
    dest="unique",
    action="store_true",
    help="Don't allow sentences in training data.")
  return parser

if __name__ == "__main__":
  main()
